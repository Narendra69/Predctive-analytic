{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIWBAtdPKAJ8Zgm6UMiQlH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Narendra69/Predctive-analytic/blob/credit-card-fraud-detection/Credit_card_fraud_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIIPsOPhPIFe",
        "outputId": "e70fb729-c49d-46d9-fd3d-a9c6a1d52a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Head ---\n",
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62    0.0  \n",
            "1  0.125895 -0.008983  0.014724    2.69    0.0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66    0.0  \n",
            "3 -0.221929  0.062723  0.061458  123.50    0.0  \n",
            "4  0.502292  0.219422  0.215153   69.99    0.0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "\n",
            "--- Data Description ---\n",
            "               Time            V1            V2            V3            V4  \\\n",
            "count  81486.000000  81486.000000  81486.000000  81486.000000  81486.000000   \n",
            "mean   37720.489237     -0.260081     -0.038216      0.678510      0.162804   \n",
            "std    15337.372455      1.887677      1.677784      1.381217      1.366843   \n",
            "min        0.000000    -56.407510    -72.715728    -33.680984     -5.172595   \n",
            "25%    30993.250000     -1.021597     -0.601897      0.185747     -0.726251   \n",
            "50%    40377.000000     -0.252385      0.069259      0.763882      0.182415   \n",
            "75%    49669.000000      1.153571      0.721301      1.393502      1.043373   \n",
            "max    58953.000000      1.960497     18.902453      4.226108     16.715537   \n",
            "\n",
            "                 V5            V6            V7            V8            V9  \\\n",
            "count  81485.000000  81485.000000  81485.000000  81485.000000  81485.000000   \n",
            "mean      -0.277340      0.095347     -0.114263      0.053242     -0.013048   \n",
            "std        1.382213      1.305163      1.245783      1.230792      1.141597   \n",
            "min      -42.147898    -26.160506    -31.764946    -73.216718     -9.283925   \n",
            "25%       -0.896819     -0.643383     -0.604700     -0.141111     -0.687376   \n",
            "50%       -0.310669     -0.154638     -0.074211      0.069391     -0.091504   \n",
            "75%        0.257987      0.490507      0.415906      0.350189      0.618288   \n",
            "max       34.801666     22.529298     36.677268     20.007208     10.392889   \n",
            "\n",
            "       ...           V21           V22           V23           V24  \\\n",
            "count  ...  81485.000000  81485.000000  81485.000000  81485.000000   \n",
            "mean   ...     -0.029572     -0.105034     -0.037965      0.008193   \n",
            "std    ...      0.737095      0.635874      0.630980      0.595548   \n",
            "min    ...    -34.830382    -10.933144    -26.751119     -2.836627   \n",
            "25%    ...     -0.224303     -0.524090     -0.178476     -0.322855   \n",
            "50%    ...     -0.058329     -0.080163     -0.050352      0.064215   \n",
            "75%    ...      0.117954      0.310647      0.080524      0.405522   \n",
            "max    ...     22.614889     10.503090     18.946734      4.014444   \n",
            "\n",
            "                V25           V26           V27           V28        Amount  \\\n",
            "count  81485.000000  81485.000000  81485.000000  81485.000000  81485.000000   \n",
            "mean       0.134780      0.026018      0.002117      0.002177     98.031261   \n",
            "std        0.441334      0.497576      0.390821      0.331090    269.360851   \n",
            "min       -7.495741     -2.534330     -9.390980     -9.617915      0.000000   \n",
            "25%       -0.129174     -0.326649     -0.063306     -0.005866      7.680000   \n",
            "50%        0.173291     -0.073305      0.009303      0.022845     26.980000   \n",
            "75%        0.422133      0.306652      0.082357      0.075926     89.700000   \n",
            "max        5.525093      3.517346     12.152401     33.847808  19656.530000   \n",
            "\n",
            "              Class  \n",
            "count  81485.000000  \n",
            "mean       0.002430  \n",
            "std        0.049234  \n",
            "min        0.000000  \n",
            "25%        0.000000  \n",
            "50%        0.000000  \n",
            "75%        0.000000  \n",
            "max        1.000000  \n",
            "\n",
            "[8 rows x 31 columns]\n",
            "\n",
            "--- Class Distribution ---\n",
            "Class\n",
            "0.0    81287\n",
            "1.0      198\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Legitimate Transactions (Class 0): 81287\n",
            "Fraudulent Transactions (Class 1): 198\n",
            "Percentage of Fraud: 0.2430%\n",
            "------------------------------\n",
            "\n",
            "--- Training Logistic Regression (Baseline) ---\n",
            "\n",
            "--- Logistic Regression Results ---\n",
            "Confusion Matrix:\n",
            "[[16249     8]\n",
            " [   15    25]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not Fraud (0)       1.00      1.00      1.00     16257\n",
            "    Fraud (1)       0.76      0.62      0.68        40\n",
            "\n",
            "     accuracy                           1.00     16297\n",
            "    macro avg       0.88      0.81      0.84     16297\n",
            " weighted avg       1.00      1.00      1.00     16297\n",
            "\n",
            "\n",
            "--- Training Random Forest Classifier ---\n",
            "\n",
            "--- Random Forest Results ---\n",
            "Confusion Matrix:\n",
            "[[16256     1]\n",
            " [    9    31]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Not Fraud (0)       1.00      1.00      1.00     16257\n",
            "    Fraud (1)       0.97      0.78      0.86        40\n",
            "\n",
            "     accuracy                           1.00     16297\n",
            "    macro avg       0.98      0.89      0.93     16297\n",
            " weighted avg       1.00      1.00      1.00     16297\n",
            "\n",
            "\n",
            "--- Model Comparison Summary ---\n",
            "Logistic Regression caught 62.50% of the fraud cases in the test set.\n",
            "Random Forest caught 77.50% of the fraud cases in the test set.\n",
            "\n",
            "Interpretation:\n",
            "The Logistic Regression model has high precision but very poor recall for fraud cases. It correctly identified only a portion of the fraudulent transactions.\n",
            "The Random Forest model, especially with `class_weight='balanced'`, performs much better. Its recall is significantly higher, meaning it successfully identified a much larger percentage of the actual fraud cases, even if it meant incorrectly flagging a few more legitimate transactions (lower precision).\n",
            "In fraud detection, high recall is often the primary goal.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# --- 1. Load and Explore the Data ---\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "data = pd.read_csv('/content/creditcard.csv', on_bad_lines='skip') # Added on_bad_lines='skip'\n",
        "\n",
        "\n",
        "print(\"--- Data Head ---\")\n",
        "print(data.head())\n",
        "print(\"\\n--- Data Description ---\")\n",
        "print(data.describe())\n",
        "\n",
        "# Check for the class imbalance\n",
        "print(\"\\n--- Class Distribution ---\")\n",
        "class_distribution = data['Class'].value_counts()\n",
        "print(class_distribution)\n",
        "print(f\"\\nLegitimate Transactions (Class 0): {class_distribution[0]}\")\n",
        "print(f\"Fraudulent Transactions (Class 1): {class_distribution[1]}\")\n",
        "print(f\"Percentage of Fraud: {class_distribution[1] / len(data) * 100:.4f}%\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# --- 2. Pre-processing ---\n",
        "\n",
        "# The 'Time' and 'Amount' columns are not scaled like the others (V1, V2, etc.).\n",
        "# We'll scale them to prevent them from overly influencing the model.\n",
        "scaler = StandardScaler()\n",
        "data['scaled_Amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))\n",
        "# We can drop the original 'Time' and 'Amount' columns\n",
        "data = data.drop(['Time', 'Amount'], axis=1)\n",
        "\n",
        "# Drop rows with missing values in the 'Class' column\n",
        "data.dropna(subset=['Class'], inplace=True)\n",
        "\n",
        "# --- 3. Prepare Data for Modeling ---\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# We use 'stratify=y' to ensure the class distribution is the same in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# --- 4. Train a Baseline Model (Logistic Regression) ---\n",
        "print(\"\\n--- Training Logistic Regression (Baseline) ---\")\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Logistic Regression Results ---\")\n",
        "print(\"Confusion Matrix:\")\n",
        "# Note: In a confusion matrix, the rows are the actual classes and columns are the predicted classes.\n",
        "# [[True Negatives, False Positives],\n",
        "#  [False Negatives, True Positives]]\n",
        "print(confusion_matrix(y_test, y_pred_lr))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_lr, target_names=['Not Fraud (0)', 'Fraud (1)']))\n",
        "\n",
        "\n",
        "# --- 5. Train an Advanced Model (Random Forest) ---\n",
        "# Random Forest is better for complex, non-linear problems and imbalanced data.\n",
        "# `class_weight='balanced'` tells the model to pay more attention to the minority class (fraud).\n",
        "print(\"\\n--- Training Random Forest Classifier ---\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1  # Use all available CPU cores\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Random Forest Results ---\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=['Not Fraud (0)', 'Fraud (1)']))\n",
        "\n",
        "\n",
        "# --- 6. Summary and Interpretation ---\n",
        "print(\"\\n--- Model Comparison Summary ---\")\n",
        "# For fraud (class 1):\n",
        "lr_recall = confusion_matrix(y_test, y_pred_lr)[1, 1] / (confusion_matrix(y_test, y_pred_lr)[1, 1] + confusion_matrix(y_test, y_pred_lr)[1, 0])\n",
        "rf_recall = confusion_matrix(y_test, y_pred_rf)[1, 1] / (confusion_matrix(y_test, y_pred_rf)[1, 1] + confusion_matrix(y_test, y_pred_rf)[1, 0])\n",
        "\n",
        "print(f\"Logistic Regression caught {lr_recall*100:.2f}% of the fraud cases in the test set.\")\n",
        "print(f\"Random Forest caught {rf_recall*100:.2f}% of the fraud cases in the test set.\")\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"The Logistic Regression model has high precision but very poor recall for fraud cases. It correctly identified only a portion of the fraudulent transactions.\")\n",
        "print(\"The Random Forest model, especially with `class_weight='balanced'`, performs much better. Its recall is significantly higher, meaning it successfully identified a much larger percentage of the actual fraud cases, even if it meant incorrectly flagging a few more legitimate transactions (lower precision).\")\n",
        "print(\"In fraud detection, high recall is often the primary goal.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# --- This part is the same as before, to get a trained model ---\n",
        "# --- In a real application, you would save and load the model, but for this example, we'll just retrain it. ---\n",
        "\n",
        "# 1. Load data\n",
        "try:\n",
        "    df = pd.read_csv('/content/creditcard.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'creditcard.csv' not found. Please place it in the directory.\")\n",
        "    exit()\n",
        "\n",
        "# 2. Pre-process\n",
        "# We need to create and fit the scaler on the full dataset so we can use it later\n",
        "scaler = StandardScaler()\n",
        "df['scaled_Amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
        "df = df.drop(['Time', 'Amount'], axis=1)\n",
        "\n",
        "# 3. Prepare data and train the model\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# For this example, we'll train on the full dataset to make the model as robust as possible\n",
        "# In a real scenario, you'd train on your training set and save the model and the scaler\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)\n",
        "print(\"Training the final model on all available data...\")\n",
        "rf_model.fit(X, y)\n",
        "print(\"Model training complete.\\n\")\n",
        "\n",
        "\n",
        "# --- THIS IS THE NEW PART: CHECKING A SINGLE TRANSACTION ---\n",
        "\n",
        "# 4. Create hypothetical new transactions to check\n",
        "# The features V1, V2, etc., are principal components, so we'll use values from the existing dataset.\n",
        "# We'll grab a known non-fraudulent and a known fraudulent transaction to simulate.\n",
        "known_non_fraud_row = df[df['Class'] == 0].iloc[0].drop('Class')\n",
        "known_fraud_row = df[df['Class'] == 1].iloc[0].drop('Class')\n",
        "\n",
        "# Let's create a custom \"suspicious\" transaction.\n",
        "# We'll use the non-fraud row as a base and change the amount to something very large.\n",
        "# Note: The 'scaled_Amount' is what matters. A raw amount of $5000 would be scaled.\n",
        "# We will manually scale it using the *already fitted* scaler.\n",
        "high_amount = 5000\n",
        "scaled_high_amount = scaler.transform(np.array([[high_amount]]))[0][0]\n",
        "\n",
        "suspicious_transaction_dict = known_non_fraud_row.to_dict()\n",
        "suspicious_transaction_dict['scaled_Amount'] = scaled_high_amount\n",
        "# Let's also change a few V-features to make it more anomalous\n",
        "suspicious_transaction_dict['V4'] = 5.5\n",
        "suspicious_transaction_dict['V11'] = 4.2\n",
        "\n",
        "\n",
        "# 5. Format the new data for the model\n",
        "# The model expects a pandas DataFrame with the columns in the correct order.\n",
        "normal_transaction = pd.DataFrame([known_non_fraud_row.to_dict()])\n",
        "suspicious_transaction = pd.DataFrame([suspicious_transaction_dict])\n",
        "\n",
        "# Ensure column order is the same as the training data (X.columns)\n",
        "normal_transaction = normal_transaction[X.columns]\n",
        "suspicious_transaction = suspicious_transaction[X.columns]\n",
        "\n",
        "\n",
        "# 6. Make Predictions\n",
        "print(\"--- Checking a known NORMAL transaction ---\")\n",
        "prediction_normal = rf_model.predict(normal_transaction)\n",
        "prediction_proba_normal = rf_model.predict_proba(normal_transaction)\n",
        "\n",
        "print(f\"Prediction (0=Normal, 1=Fraud): {prediction_normal[0]}\")\n",
        "# predict_proba returns probabilities for [Class 0, Class 1]\n",
        "print(f\"Fraud Probability: {prediction_proba_normal[0][1] * 100:.2f}%\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Checking a SUSPICIOUS transaction ---\")\n",
        "prediction_suspicious = rf_model.predict(suspicious_transaction)\n",
        "prediction_proba_suspicious = rf_model.predict_proba(suspicious_transaction)\n",
        "\n",
        "print(f\"Prediction (0=Normal, 1=Fraud): {prediction_suspicious[0]}\")\n",
        "print(f\"Fraud Probability: {prediction_proba_suspicious[0][1] * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fmmbz8YTQWq",
        "outputId": "41e13a4b-f5e7-4890-96ed-156cf7c603f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the final model on all available data...\n",
            "Model training complete.\n",
            "\n",
            "--- Checking a known NORMAL transaction ---\n",
            "Prediction (0=Normal, 1=Fraud): 0\n",
            "Fraud Probability: 0.00%\n",
            "\n",
            "--- Checking a SUSPICIOUS transaction ---\n",
            "Prediction (0=Normal, 1=Fraud): 0\n",
            "Fraud Probability: 0.00%\n"
          ]
        }
      ]
    }
  ]
}